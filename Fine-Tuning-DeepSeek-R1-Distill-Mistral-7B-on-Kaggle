{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mehrdadal2023/deepseek-r1-distill-mistral-7b-instruct-on-kaggle?scriptVersionId=223540228\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"![Hugging Face](https://huggingface.co/front/assets/huggingface_logo-noborder.svg)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# üèóÔ∏è Working with [DeepSeek-R1-Distill-Qwen-7B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)\n\nThe **DeepSeek-R1-Distill-Qwen-7B** model is a distilled version of DeepSeek's R1 reasoning model, fine-tuned for **advanced reasoning and chain-of-thought tasks**. It supports a **128k token context length**, making it well-suited for complex text generation.\n\n## üöÄ **Key Features**\n- **üîç Distilled Model:** Retains strong reasoning abilities from DeepSeek-R1 in a more compact form.\n- **üìú Extended Context Length:** Handles up to **128,000 tokens**, allowing for **better long-context understanding**.\n- **üß† Optimized for Reasoning:** Fine-tuned for **structured thought processes** and **logical inference**.\n\n## üõ†Ô∏è **How to Use**\n### **üì¶ Install Dependencies**\n```bash\npip install torch transformers accelerate\n","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom accelerate import load_checkpoint_and_dispatch\n\n# Set environment variables for CUDA devices\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Specify the model name\n\n# https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\nmodel_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\ncache_dir = \"/kaggle/temp\"  # Use Kaggle's temp storage to avoid disk space issues\n\n# ‚úÖ Load the tokenizer with caching\ntokenizer = AutoTokenizer.from_pretrained(\n    model_name,\n    cache_dir=cache_dir,  # Store model files in Kaggle's temporary directory\n    trust_remote_code=True\n)\n\n# ‚úÖ Load the model with caching and multi-GPU execution\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    cache_dir=cache_dir,  # Cache model in temp storage\n    torch_dtype=torch.float16,  # ‚úÖ Use torch.float16 instead of string\n    device_map=\"auto\",  # Automatically distribute across available GPUs\n    trust_remote_code=True\n)\n\n# Define multiple prompt variations to observe impact\nprompts = [\n    \"Solve the equation: 3x + 5 = 20. What is x?\",\n    \"What is x if 3x + 5 = 20?\",\n    \"Find x in the equation: 3x + 5 = 20.\",\n    \"Compute x given 3x + 5 = 20.\",\n    \"If 3x + 5 equals 20, what does x equal?\",\n    \"Determine the numerical value of x in 3x + 5 = 20.\",\n    \"Can you solve for x: 3x + 5 = 20?\",\n    \"What is the solution to 3x + 5 = 20?\"\n]\n\n# Tokenize the prompts\ninputs = tokenizer(prompts, padding=True, return_tensors=\"pt\").to(\"cuda\")\n\nprint(\"üî¢ **Tokenized Input (IDs):**\")\nprint(inputs[\"input_ids\"])  # Shows numerical tokenized representation\n\n# Perform inference\nwith torch.no_grad():\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=1024,  # Limit the number of generated tokens\n        temperature=0.7,  # Sampling temperature\n        top_p=0.9,  # Nucleus sampling\n        repetition_penalty=1.1  # Penalize repetition\n    )\n\n# Display raw model output (before converting to text)\nprint(\"\\nüìä **Raw Model Output (IDs):**\")\nprint(outputs)  # Shows the generated token IDs before decoding\n\n# Decode and print responses\ndecoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\nfor i, response in enumerate(decoded_outputs):\n    print(f\"Response for prompt {i+1}:\\n{response}\\n\")\n    print(\"***********************************************\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T10:57:58.202785Z","iopub.execute_input":"2025-02-20T10:57:58.203116Z","iopub.status.idle":"2025-02-20T10:59:50.63397Z","shell.execute_reply.started":"2025-02-20T10:57:58.203094Z","shell.execute_reply":"2025-02-20T10:59:50.632826Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del outputs,inputs,tokenizer,model\nimport gc, torch\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T10:59:50.635223Z","iopub.execute_input":"2025-02-20T10:59:50.635862Z","iopub.status.idle":"2025-02-20T10:59:51.274625Z","shell.execute_reply.started":"2025-02-20T10:59:50.635834Z","shell.execute_reply":"2025-02-20T10:59:51.273642Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üèÜ **Mistral-7B-Instruct-v0.3**  \nüöÄ **[View on Hugging Face](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3)**  \n\n## üìå **Overview**  \n**Mistral-7B-Instruct-v0.3** is a **fine-tuned version** of Mistral-7B, optimized for **instruction-following tasks** and **chat-based interactions**. It builds upon Mistral-7B‚Äôs **efficient architecture**, delivering **high-quality reasoning and conversational abilities** while maintaining a **small model size (7B parameters)** for improved performance.\n\n## ‚ö° **Key Features**\n- **üß† Strong Instruction-Following:** Optimized for handling a wide range of user queries, including **reasoning, coding, and knowledge-based tasks**.\n- **üîç Enhanced Context Understanding:** Works well with **long-form text generation** while maintaining coherence.\n- **üìè Compact Yet Powerful (7B Parameters):** **Balances speed and performance**, making it a great alternative to larger models.\n- **üñ•Ô∏è Efficient Execution:** Supports **multi-GPU execution** with **float16 precision for reduced memory usage**.\n  \n## üõ† **How to Use**\n### **1Ô∏è‚É£ Install Dependencies**\n```bash\npip install torch transformers accelerate\n","metadata":{}},{"cell_type":"markdown","source":"## üîë Step 1: Log in to Hugging Face from Kaggle\nSince Kaggle does not have an interactive login like local machines, you need to use your **Hugging Face token** for authentication.\n\n1. Go to **[Hugging Face Tokens](https://huggingface.co/settings/tokens)**\n2. **Create a new access token** with **\"read\" permissions**\n3. **Copy the token**\n","metadata":{"execution":{"iopub.status.busy":"2025-02-20T10:51:06.628649Z","iopub.execute_input":"2025-02-20T10:51:06.628937Z","iopub.status.idle":"2025-02-20T10:51:06.635241Z","shell.execute_reply.started":"2025-02-20T10:51:06.628916Z","shell.execute_reply":"2025-02-20T10:51:06.634065Z"}}},{"cell_type":"code","source":"from huggingface_hub import login\n\n# Replace 'your_huggingface_token_here' with your actual token\nlogin(\"hf_X.............................\")  # replace it with your read token\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T11:12:00.34282Z","iopub.execute_input":"2025-02-20T11:12:00.343082Z","iopub.status.idle":"2025-02-20T11:12:00.385425Z","shell.execute_reply.started":"2025-02-20T11:12:00.343043Z","shell.execute_reply":"2025-02-20T11:12:00.384811Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n## ‚úÖ Step 2: Accept Model Terms on Hugging Face\nSome Hugging Face models require **explicit access approval** before you can use them.\n\n1. Go to the **[Mistral-7B-Instruct-v0.3 Model Page](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3)**\n2. Click **\"Request Access\"** and accept the terms\n3. Wait for **access approval** (this might take some time)\n","metadata":{"execution":{"iopub.status.busy":"2025-02-20T10:53:05.723453Z","iopub.execute_input":"2025-02-20T10:53:05.723807Z","iopub.status.idle":"2025-02-20T10:53:05.729916Z","shell.execute_reply.started":"2025-02-20T10:53:05.723784Z","shell.execute_reply":"2025-02-20T10:53:05.728709Z"}}},{"cell_type":"code","source":"import os\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Set environment variables for CUDA devices\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# üîπ Specify the Mistral v3 model name\nmodel_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\ncache_dir = \"/kaggle/temp\"  # ‚úÖ Cache model in Kaggle‚Äôs temp directory to avoid space issues\n\n# ‚úÖ Load the tokenizer with authentication\ntokenizer = AutoTokenizer.from_pretrained(\n    model_name,\n    cache_dir=cache_dir,\n    use_auth_token=True,\n    trust_remote_code=True\n)\n\n# üîπ Fix: Set a padding token if it doesn't exist\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token  # Use EOS token as padding\n\n# ‚úÖ Load the model with caching and multi-GPU execution\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    cache_dir=cache_dir,\n    torch_dtype=torch.float16,  # ‚úÖ Use torch.float16 for efficiency\n    device_map=\"auto\",  # ‚úÖ Automatically distribute across available GPUs\n    use_auth_token=True,\n    trust_remote_code=True\n)\n\n# Step 4: Define multiple prompts to test the model\nprompts = [\n    \"Solve the equation: 3x + 5 = 20. What is x?\",\n    \"What is x if 3x + 5 = 20?\",\n    \"Find x in the equation: 3x + 5 = 20.\",\n    \"Compute x given 3x + 5 = 20.\",\n    \"If 3x + 5 equals 20, what does x equal?\",\n    \"Determine the numerical value of x in 3x + 5 = 20.\",\n    \"Can you solve for x: 3x + 5 = 20?\",\n    \"What is the solution to 3x + 5 = 20?\"\n]\n\n# Step 5: Tokenize the prompts\ninputs = tokenizer(prompts, padding=True, return_tensors=\"pt\").to(\"cuda\")\n\n# üî¢ Display tokenized input (series of numbers)\nprint(\"üî¢ **Tokenized Input (IDs):**\")\nprint(inputs[\"input_ids\"])  # Shows numerical tokenized representation\n\n# Step 6: Perform inference\nwith torch.no_grad():\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=1024,  # Limit the number of generated tokens\n        temperature=0.7,  # Sampling temperature\n        top_p=0.9,  # Nucleus sampling\n        repetition_penalty=1.1  # Penalize repetition\n    )\n\n# üìä Display raw model output (before converting to text)\nprint(\"\\nüìä **Raw Model Output (IDs):**\")\nprint(outputs)  # Shows the generated token IDs before decoding\n\n# Step 7: Decode and print responses\ndecoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\nfor i, response in enumerate(decoded_outputs):\n    print(f\"Response for prompt {i+1}:\\n{response}\\n\")\n    print(\"***********************************************\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T11:10:39.936678Z","iopub.execute_input":"2025-02-20T11:10:39.937044Z","iopub.status.idle":"2025-02-20T11:12:00.33918Z","shell.execute_reply.started":"2025-02-20T11:10:39.937018Z","shell.execute_reply":"2025-02-20T11:12:00.338046Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}