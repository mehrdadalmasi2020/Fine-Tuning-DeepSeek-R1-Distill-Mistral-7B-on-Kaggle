# ğŸš€ Fine-Tuning DeepSeek-R1-Distill-Mistral-7B on Kaggle

This notebook demonstrates how to fine-tune and run inference with **DeepSeek-R1-Distill-Qwen-7B** and **Mistral-7B-Instruct** models using Hugging Face's `transformers` library. It focuses on optimizing **large language models (LLMs)** for reasoning, instruction-following, and conversational AI.

---

## ğŸ—ï¸ Model Overview
- âœ… **DeepSeek-R1-Distill-Qwen-7B**: A distilled version of DeepSeekâ€™s R1 model, optimized for **logical reasoning and long-context tasks** (supports **128k tokens**).
- âœ… **Mistral-7B-Instruct**: Fine-tuned for **instruction-following** with strong **conversation and coding capabilities**.

---

## ğŸ“¥ Setup & Dependencies
- Installs required libraries:  
pip install torch transformers accelerate


- âœ… Configures **multi-GPU execution**.
- âœ… Enables **float16 precision** for memory efficiency.

---

## ğŸ”„ Model Loading & Inference
- âœ… Loads **pretrained models** from Hugging Face.
- âœ… Uses **tokenizers** for text input processing.
- âœ… Performs **inference with various prompts**, including **math problem-solving and reasoning tasks**.

---

## ğŸ¯ Fine-Tuning & Optimization
- âœ… Fine-tunes the model on **Kaggle GPUs**.
- âœ… Implements **gradient accumulation & mixed precision training**.
- âœ… Uses **cached model storage** to optimize disk usage on Kaggle.

---

## ğŸ“Š Evaluation & Output
- âœ… **Runs inference** on structured reasoning tasks.
- âœ… Displays **model-generated responses** for multiple prompts.

---

## ğŸ’¾ Cleanup & Resource Management
- âœ… Clears GPU cache (`torch.cuda.empty_cache()`) to prevent memory overflow.
- âœ… Deletes large variables after execution to optimize memory.

---

## ğŸ¯ Conclusion
This notebook provides a **hands-on guide** to running **DeepSeek-R1-Distill-Qwen-7B** and **Mistral-7B-Instruct** models on Kaggle. The models are **efficient, powerful, and suitable for advanced NLP tasks**, including **reasoning, chat-based AI, and long-context understanding**.

ğŸš€ *Ideal for AI researchers and developers working on cutting-edge language model applications!*
