# ğŸš€ DeepSeek-R1-Distill_Mistral-7B-Instruct_on_Kaggle


# ğŸ—ï¸ Working with [DeepSeek-R1-Distill-Qwen-7B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)

The **DeepSeek-R1-Distill-Qwen-7B** model is a distilled version of DeepSeek's R1 reasoning model, fine-tuned for **advanced reasoning and chain-of-thought tasks**. It supports a **128k token context length**, making it well-suited for complex text generation.

## ğŸš€ **Key Features**
- **ğŸ” Distilled Model:** Retains strong reasoning abilities from DeepSeek-R1 in a more compact form.
- **ğŸ“œ Extended Context Length:** Handles up to **128,000 tokens**, allowing for **better long-context understanding**.
- **ğŸ§  Optimized for Reasoning:** Fine-tuned for **structured thought processes** and **logical inference**.

## ğŸ› ï¸ **How to Use**
### **ğŸ“¦ Install Dependencies**
```bash
pip install torch transformers accelerate
```

# ğŸ† **Mistral-7B-Instruct-v0.3**  
ğŸš€ **[View on Hugging Face](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3)**  

## ğŸ“Œ **Overview**  
**Mistral-7B-Instruct-v0.3** is a **fine-tuned version** of Mistral-7B, optimized for **instruction-following tasks** and **chat-based interactions**. It builds upon Mistral-7Bâ€™s **efficient architecture**, delivering **high-quality reasoning and conversational abilities** while maintaining a **small model size (7B parameters)** for improved performance.

## âš¡ **Key Features**
- **ğŸ§  Strong Instruction-Following:** Optimized for handling a wide range of user queries, including **reasoning, coding, and knowledge-based tasks**.
- **ğŸ” Enhanced Context Understanding:** Works well with **long-form text generation** while maintaining coherence.
- **ğŸ“ Compact Yet Powerful (7B Parameters):** **Balances speed and performance**, making it a great alternative to larger models.
- **ğŸ–¥ï¸ Efficient Execution:** Supports **multi-GPU execution** with **float16 precision for reduced memory usage**.
  
## ğŸ›  **How to Use**
### **1ï¸âƒ£ Install Dependencies**
```bash
pip install torch transformers accelerate
```
