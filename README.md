# 🚀 DeepSeek-R1-Distill_Mistral-7B-Instruct_on_Kaggle


# 🏗️ Working with [DeepSeek-R1-Distill-Qwen-7B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)

The **DeepSeek-R1-Distill-Qwen-7B** model is a distilled version of DeepSeek's R1 reasoning model, fine-tuned for **advanced reasoning and chain-of-thought tasks**. It supports a **128k token context length**, making it well-suited for complex text generation.

## 🚀 **Key Features**
- **🔍 Distilled Model:** Retains strong reasoning abilities from DeepSeek-R1 in a more compact form.
- **📜 Extended Context Length:** Handles up to **128,000 tokens**, allowing for **better long-context understanding**.
- **🧠 Optimized for Reasoning:** Fine-tuned for **structured thought processes** and **logical inference**.

## 🛠️ **How to Use**
### **📦 Install Dependencies**
```bash
pip install torch transformers accelerate
```

# 🏆 **Mistral-7B-Instruct-v0.3**  
🚀 **[View on Hugging Face](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3)**  

## 📌 **Overview**  
**Mistral-7B-Instruct-v0.3** is a **fine-tuned version** of Mistral-7B, optimized for **instruction-following tasks** and **chat-based interactions**. It builds upon Mistral-7B’s **efficient architecture**, delivering **high-quality reasoning and conversational abilities** while maintaining a **small model size (7B parameters)** for improved performance.

## ⚡ **Key Features**
- **🧠 Strong Instruction-Following:** Optimized for handling a wide range of user queries, including **reasoning, coding, and knowledge-based tasks**.
- **🔍 Enhanced Context Understanding:** Works well with **long-form text generation** while maintaining coherence.
- **📏 Compact Yet Powerful (7B Parameters):** **Balances speed and performance**, making it a great alternative to larger models.
- **🖥️ Efficient Execution:** Supports **multi-GPU execution** with **float16 precision for reduced memory usage**.
  
## 🛠 **How to Use**
### **1️⃣ Install Dependencies**
```bash
pip install torch transformers accelerate
```
